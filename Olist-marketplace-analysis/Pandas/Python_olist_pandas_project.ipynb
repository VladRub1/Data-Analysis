{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Python\n",
        "## `pandas` practice with Olist\n",
        "\n",
        "In this notebook, you can see the solution of various non-trivial tasks with the use of module `pandas`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rlek5byfvxPo"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/a/60658965/7286121\n",
        "\n",
        "from IPython.core.magic import register_cell_magic\n",
        "\n",
        "@register_cell_magic\n",
        "def write_and_run(line, cell):\n",
        "    argz = line.split()\n",
        "    file = argz[-1]\n",
        "    mode = 'w'\n",
        "    if len(argz) == 2 and argz[0] == '-a':\n",
        "        mode = 'a'\n",
        "    with open(file, mode) as f:\n",
        "        f.write(cell)\n",
        "    get_ipython().run_cell(cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63di75_vqbTp"
      },
      "source": [
        "## Data\n",
        "\n",
        "In this assignment, you will have to step into the shoes of an analyst in a Brazilian marketplace [Olist](https://olist.com/pt-br/). You need to examine the data and from it draw conclusions that will help the business flourish!\n",
        "\n",
        "The data will be downloaded in `archive.zip` after the next cell is executed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5U4xc4-9b9XR"
      },
      "outputs": [],
      "source": [
        "# !pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ItTnixKIrtHQ"
      },
      "outputs": [],
      "source": [
        "# This code that will be in each test, do not change names of variables\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import wget\n",
        "\n",
        "url = 'https://github.com/Palladain/Deep_Python/raw/main/Homeworks/Homework_1/archive.zip'\n",
        "filename = wget.download(url)\n",
        "\n",
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')\n",
        "\n",
        "customers = pd.read_csv('olist_customers_dataset.csv')\n",
        "location = pd.read_csv('olist_geolocation_dataset.csv')\n",
        "items = pd.read_csv('olist_order_items_dataset.csv')\n",
        "payments = pd.read_csv('olist_order_payments_dataset.csv')\n",
        "reviews = pd.read_csv('olist_order_reviews_dataset.csv')\n",
        "orders = pd.read_csv('olist_orders_dataset.csv')\n",
        "products = pd.read_csv('olist_products_dataset.csv')\n",
        "translation = pd.read_csv('product_category_name_translation.csv')\n",
        "sellers = pd.read_csv('olist_sellers_dataset.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLos28bYrbR2"
      },
      "source": [
        "Van is given 9 datasets that contain all the data for 100,000 orders from all over Brazil. To make your life easier, here are the links to these datasets (the file `product_category_name_translation` is a translation of the category names from Portuguese to English)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXXie4hQrdhI"
      },
      "source": [
        "![](https://i.imgur.com/HRhd2Y0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pUMuhE1rWhb"
      },
      "source": [
        "All right, let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnjVNWlFrVKG"
      },
      "source": [
        "### Task №1\n",
        "\n",
        "Determine:\n",
        "\n",
        "* Number of items\n",
        "\n",
        "* Average item price (item price = average of prices in items dataset)\n",
        "\n",
        "by category (all categories must be in English)\n",
        "\n",
        "The table you should have:\n",
        "\n",
        "```\n",
        "category | products | price\n",
        "\n",
        "value    | value    | value\n",
        "```\n",
        "\n",
        "**Note:**\n",
        "\n",
        "For the category `portateis_cozinha_e_preparadores_de_alimentos` translation is `portable kitchen and food preparers`\n",
        "\n",
        "For the category `pc_gamer`, translation `PC Gamer`.\n",
        "\n",
        "You need to add a translation for them separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PWVaTwBFrkFj"
      },
      "outputs": [],
      "source": [
        "%%write_and_run task_1.py\n",
        "\n",
        "# important! all dependencies that you use (if you add new ones) in this class must be explicitly duplicated in this cell\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def task_1(translation, items, products):\n",
        "    # creating a dataframe with additional translations to add them\n",
        "    translation_to_add = pd.DataFrame(\n",
        "            [['portateis_cozinha_e_preparadores_de_alimentos', 'portable kitchen and food preparers'], \n",
        "             ['pc_gamer', 'PC Gamer']],\n",
        "        columns=translation.columns)\n",
        "    # supplementing the dataframe with translations, so that I can join everything I need at once\n",
        "    translation_full = pd.concat([translation, translation_to_add], ignore_index=True)\n",
        "    # checking that there are 2 more categories in the translations\n",
        "    assert translation_full.shape[0] == translation.shape[0] + 2\n",
        "\n",
        "    # counting the average price of products\n",
        "    items_price = items.groupby('product_id').agg({'price': 'mean'}).reset_index()\n",
        "    # join aggregate info on products and aitems\n",
        "    products_and_items = pd.merge(products, items_price, how='inner', on='product_id')\n",
        "    # add translation. LEFT JOIN, because the product_category_name field is NULL\n",
        "    products_items_and_translation = pd.merge(products_and_items, translation_full, how='left', on='product_category_name')\n",
        "    # final aggregations\n",
        "    res = products_items_and_translation.groupby('product_category_name_english').agg({'product_id': 'nunique', 'price': 'mean'}).reset_index()\n",
        "    # rename as required\n",
        "    res = res.rename({'product_category_name_english': 'category',\n",
        "                      'product_id': 'products'}, axis=1)\n",
        "    \n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VgSA5JAjxngq"
      },
      "outputs": [],
      "source": [
        "# Checks\n",
        "\n",
        "res = task_1(translation, items, products)\n",
        "\n",
        "assert res[res.category == 'portable kitchen and food preparers'].price.values[0] == 186.996\n",
        "assert len(res) == 73\n",
        "assert len(res.drop_duplicates()) == 73\n",
        "assert res[res.category == 'drinks'].products.values[0] == 81\n",
        "assert res.products.sum() == 32341\n",
        "assert res.price.sum() == 12459.751444351941\n",
        "assert res[res.category == 'home_confort'].price.values[0] == 185.56926417326417"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUh0q89ztKMV"
      },
      "source": [
        "### Task №2\n",
        "\n",
        "Identify for each salesperson the main category of their sales (categories should be in English)\n",
        "\n",
        "The table you should have:\n",
        "\n",
        "```\n",
        "seller_id | category\n",
        "\n",
        "value     | value\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YCKMxSNtzdxO"
      },
      "outputs": [],
      "source": [
        "%%write_and_run task_2.py\n",
        "\n",
        "# important! all dependencies that you use (if you add new ones) in this class must be explicitly duplicated in this cell\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def task_2(translation, products, items):\n",
        "    # creating a dataframe with additional translations to add them\n",
        "    translation_to_add = pd.DataFrame(\n",
        "            [['portateis_cozinha_e_preparadores_de_alimentos', 'portable kitchen and food preparers'], \n",
        "                ['pc_gamer', 'PC Gamer']],\n",
        "        columns=translation.columns)\n",
        "    # supplementing the dataframe with translations, so that I can join everything I need at once\n",
        "    translation_full = pd.concat([translation, translation_to_add], ignore_index=True)\n",
        "    # combine products and category translations\n",
        "    translation_full_products = pd.merge(products, translation_full, how='left', on='product_category_name')\n",
        "    # merging the last dataset and the aitems\n",
        "    items_translation_full_products = pd.merge(items, translation_full_products, how='inner', on='product_id')\n",
        "    # group by seller_id and product_category_name_english, count the number of orders, \n",
        "    # sort by order count within a category\n",
        "    grouped = items_translation_full_products.groupby(['seller_id', \n",
        "                                                       'product_category_name_english']\n",
        "                                                       ).agg({\"order_item_id\": \"count\"}).reset_index().sort_values('order_item_id', ascending=False)\n",
        "    # determine the most popular category for each seller_id\n",
        "    res = grouped.groupby('seller_id').agg({'product_category_name_english': 'first'}).reset_index()\n",
        "    # renaming it according to the assignment\n",
        "    res = res.rename({'product_category_name_english': 'category'}, axis=1)\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jfx4Lfn10Tic"
      },
      "outputs": [],
      "source": [
        "# Checks\n",
        "\n",
        "res = task_2(translation, products, items)\n",
        "assert res[res.seller_id == 'e3e15e2c0b9700561efac21c6be48066'].category.values[0] == 'housewares'\n",
        "assert res[res.seller_id == '2f73e04d12cdf0c945ded66bb3fcf6c7'].category.values[0] == 'garden_tools'\n",
        "assert len(res) == len(res.drop_duplicates())\n",
        "assert len(res) == 3035\n",
        "assert len(res[res.category == 'telephony']) == 66\n",
        "assert list(np.sort(res.groupby(\"category\").agg({\"seller_id\": \"nunique\"}).seller_id.values)) == [  1,   1,   1,   1,   1,   2,   2,   2,   2,   2,   3,   3,   4,\n",
        "         4,   4,   4,   5,   5,   5,   5,   5,   5,   6,   6,   6,   7,\n",
        "         8,  10,  12,  13,  13,  13,  14,  14,  14,  15,  16,  17,  17,\n",
        "        17,  19,  20,  20,  20,  21,  22,  26,  37,  37,  43,  46,  51,\n",
        "        54,  59,  66,  66,  78,  87,  87,  99, 101, 116, 125, 156, 216,\n",
        "       224, 256, 288, 310]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQemvudwt4mt"
      },
      "source": [
        "### Task №1\n",
        "\n",
        "Print the percentage of money spent by each state (money spent - sum of money by orders delivered, sum by price and freight_value)\n",
        "\n",
        "*Note:* State breakdown is by buyer's state, percentage is a number between 0 and 1\n",
        "\n",
        "\n",
        "The table you should have:\n",
        "\n",
        "```\n",
        "state | perc\n",
        "\n",
        "value | value\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WH5QCAJi3sWk"
      },
      "outputs": [],
      "source": [
        "%%write_and_run task_3.py\n",
        "\n",
        "# important! all dependencies that you use (if you add new ones) in this class must be explicitly duplicated in this cell\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def task_3(orders, customers, items):\n",
        "    # take only orders that have been delivered\n",
        "    delivered_orders = orders[orders['order_status'] == 'delivered']\n",
        "    # merge delivered orders with aitems\n",
        "    delivered_orders_items = pd.merge(delivered_orders, items, 'inner', 'order_id')\n",
        "    # merge past dataset with clients\n",
        "    delivered_orders_items_customers = pd.merge(delivered_orders_items, customers)\n",
        "    # group info by state \n",
        "    grouped_data = delivered_orders_items_customers.groupby('customer_state').agg({\"price\": \"sum\", \"freight_value\": \"sum\"}).reset_index()\n",
        "    # get the total amount of orders: item price + shipping\n",
        "    grouped_data['total_amt'] = grouped_data['price'] + grouped_data['freight_value']\n",
        "    # get a share of every state\n",
        "    grouped_data['perc'] =  grouped_data['total_amt'] / grouped_data['total_amt'].sum()\n",
        "    # rename it and take only the columns I need\n",
        "    res = grouped_data.rename({'customer_state': 'state'}, axis=1)\n",
        "    res = res[['state', 'perc']]\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gE01dmRp5YlV"
      },
      "outputs": [],
      "source": [
        "# Checks\n",
        "\n",
        "res = task_3(orders, customers, items)\n",
        "assert np.allclose(res.perc.sum(), 1)  # in case of numerical problems\n",
        "assert res[res.state == \"RS\"].perc.values[0] == 0.055868056429816286\n",
        "assert res.sort_values(\"perc\", ascending=True).iloc[0, 1] == 0.0005862290943146945\n",
        "assert res.sort_values(\"perc\", ascending=False).iloc[0, 1] == 0.3741756035817322\n",
        "assert len(res) == 27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z5KBkyet7wB"
      },
      "source": [
        "### Task №4\n",
        "\n",
        "Determine the average purchase receipt (add a breakdown for the cost of the order itself and the cost of shipping) and the average number of items in the order.\n",
        "\n",
        "Also determine the average number of purchases per user (note the identifiers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lMqosgB-7OeQ"
      },
      "outputs": [],
      "source": [
        "%%write_and_run task_4.py\n",
        "\n",
        "# important! all dependencies that you use (if you add new ones) in this class must be explicitly duplicated in this cell\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def task_4(items, orders, customers):\n",
        "    # merge orders and items\n",
        "    orders_items = pd.merge(orders, items, 'inner', 'order_id')\n",
        "    # aggregate info, get sum of price, shipping cost, number of items in the order\n",
        "    grouped_ord = orders_items.groupby('order_id').agg({'price': 'sum', 'freight_value': 'sum', 'order_item_id': 'count'}).reset_index()\n",
        "    # counting the average price\n",
        "    price_mean = grouped_ord['price'].mean()\n",
        "    # calculating the average shipping cost\n",
        "    freight_value_mean = grouped_ord['freight_value'].mean()\n",
        "    # calculate the average number of items in an order\n",
        "    order_item_id_mean = grouped_ord['order_item_id'].mean()\n",
        "    \n",
        "    # combining orders and customers\n",
        "    orders_customers = pd.merge(orders, customers, 'inner', 'customer_id')\n",
        "    # group by customer_unique_id, count the number of unique customer_id\n",
        "    customers_data = orders_customers.groupby('customer_unique_id').agg({'customer_id': 'nunique'}).reset_index()\n",
        "    # calculate the average number of purchases per user\n",
        "    customer_id_mean = customers_data['customer_id'].mean()\n",
        "\n",
        "    return price_mean, freight_value_mean, order_item_id_mean, customer_id_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ManEB8qf7lm_"
      },
      "outputs": [],
      "source": [
        "# Checking\n",
        "\n",
        "res = task_4(items, orders, customers)\n",
        "assert res == (137.7540763788945, 22.823561713254815, 1.1417306873695092, 1.0348089410589412)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEHsUeuYuPsK"
      },
      "source": [
        "### Task №5\n",
        "\n",
        "Calculate CSAT (customer satisfaction - average review score) and display the average CSAT by day from April 2017 to April 2018.\n",
        "\n",
        "All date manipulation should be done using `datetime` and `dateutil`.\n",
        "\n",
        "The table you should get:\n",
        "\n",
        "```\n",
        "     date    | csat\n",
        "\n",
        "\"YYYY-MM-DD\" | value\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Fu31Qrod9M_4"
      },
      "outputs": [],
      "source": [
        "%%write_and_run task_5.py\n",
        "\n",
        "# important! all dependencies that you use (if you add new ones) in this class must be explicitly duplicated in this cell\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dateutil.parser import parse\n",
        "import datetime as dt\n",
        "\n",
        "def task_5(reviews):\n",
        "    # make a copy of the dataset so I don't have to change the original dataset\n",
        "    reviews_copy = reviews.copy()\n",
        "    # set the date format YYYYY-mm-dd\n",
        "    format = '%Y-%m-%d'\n",
        "    # setting the right boundaries on the dates\n",
        "    date_start = parse('2017-04-01', dayfirst=False)\n",
        "    date_finish = parse('2018-04-30', dayfirst=False)\n",
        "    # date parsing: add a column with date in datetime format\n",
        "    reviews_copy['date_datetime'] = reviews_copy['review_creation_date'].apply(lambda date: parse(date, dayfirst=False))\n",
        "    # filter only the period we need from date_start to date_finish\n",
        "    pre_need_reviews = reviews_copy[date_start <= reviews_copy['date_datetime']]\n",
        "    need_reviews = pre_need_reviews[pre_need_reviews['date_datetime'] <= date_finish]\n",
        "    # translate datetime into a string in the required format \n",
        "    need_reviews['date'] = need_reviews['date_datetime'].apply(lambda date: dt.datetime.strftime(date, format))\n",
        "    # aggregate by day, average CSAT, rename the column\n",
        "    res = need_reviews.groupby('date').agg({'review_score': 'mean'}).reset_index()\n",
        "    res = res.rename({'review_score': 'csat'}, axis=1)\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UK4r71Vh-Pfu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3288\\2128222643.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  need_reviews['date'] = need_reviews['date_datetime'].apply(lambda date: dt.datetime.strftime(date, format))\n"
          ]
        }
      ],
      "source": [
        "# Checks\n",
        "res = task_5(reviews)\n",
        "assert res.date.min() == '2017-04-01'\n",
        "assert res.date.max() == '2018-04-30'\n",
        "assert res.csat.sum() == 1551.8881071384853\n",
        "assert res[res.date == '2017-07-11'].csat.values[0] == 4.291390728476821\n",
        "assert res[res.date == '2018-02-09'].csat.values[0] == 3.992156862745098\n",
        "assert res[res.csat == 3.6814814814814816].date.values[0] == '2018-02-25'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-rho7C0uUTY"
      },
      "source": [
        "### Task №6\n",
        "\n",
        "Look at how fast users respond (do aggregation by the number of days of response) and what the average score is.\n",
        "\n",
        "All time manipulation should be done via `datetime` and `dateutil`.\n",
        "\n",
        "The table you should have:\n",
        "\n",
        "```\n",
        "days  | csat  | orders\n",
        "\n",
        "value | value | value\n",
        "```\n",
        "\n",
        "The results should be sorted by day in ascending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BCjO8WQgMeGC"
      },
      "outputs": [],
      "source": [
        "%%write_and_run task_6.py\n",
        "\n",
        "# important! all dependencies that you use (if you add new ones) in this class must be explicitly duplicated in this cell\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dateutil.parser import parse\n",
        "import datetime as dt\n",
        "\n",
        "def task_6(reviews):\n",
        "    # make a copy of the dataset so I don't have to change the original dataset\n",
        "    reviews_copy = reviews.copy()\n",
        "\n",
        "    # parsing of review and response publication dates\n",
        "    reviews_copy['review_creation_date_datetime'] = reviews_copy['review_creation_date'].apply(lambda date: parse(date, dayfirst=False))\n",
        "    reviews_copy['review_answer_timestamp_datetime'] = reviews_copy['review_answer_timestamp'].apply(lambda date: parse(date, dayfirst=False))\n",
        "    # calculate the delta between publication and response\n",
        "    reviews_copy['delta'] = reviews_copy['review_answer_timestamp_datetime'] - reviews_copy['review_creation_date_datetime']\n",
        "    # convert delta to number of days\n",
        "    reviews_copy['delta_days'] = reviews_copy['delta'].apply(lambda date: date.days)\n",
        "    # aggregate by response day: average CSAT and number of responses\n",
        "    res = reviews_copy.groupby('delta_days').agg({'review_score': 'mean', 'delta': 'count'}).reset_index()\n",
        "    # rename the columns\n",
        "    res = res.rename({'delta_days': 'days', 'review_score': 'csat', 'delta': 'orders'}, axis=1)\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "b_zq1nfxOvZa"
      },
      "outputs": [],
      "source": [
        "# Checks\n",
        "\n",
        "res = task_6(reviews)\n",
        "assert res.orders.sum() == 99224\n",
        "assert np.all(res.days.values == np.sort(res.days.values))\n",
        "assert len(res) == 214\n",
        "assert res.days.min() == 0\n",
        "assert res.days.max() == 518\n",
        "assert res[res.days == 233].csat.values[0] == 3.0\n",
        "assert res[res.days == 87].orders.values[0] == 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc-MZgP2udIi"
      },
      "source": [
        "### Task №7\n",
        "\n",
        "Highlight all orders where the order_delivered_customer_date field is not set. Replace it with the date '2999-12-31'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oqIpnXvbP3FZ"
      },
      "outputs": [],
      "source": [
        "%%write_and_run task_7.py\n",
        "\n",
        "# important! all dependencies that you use (if you add new ones) in this class must be explicitly duplicated in this cell\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def task_7(orders):\n",
        "    # I make a copy of the dataset so I don't have to change the original dataset\n",
        "    orders_copy = orders.copy()\n",
        "    # fill NaN 2999-12-31\n",
        "    orders_copy['order_delivered_customer_date'] = orders_copy['order_delivered_customer_date'].fillna('2999-12-31')\n",
        "\n",
        "    return orders_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "k6z59EMXWLkm"
      },
      "outputs": [],
      "source": [
        "# Checks\n",
        "\n",
        "res = task_7(orders)\n",
        "assert len(res[res.order_delivered_customer_date.isna()]) == 0\n",
        "assert len(res) == 99441\n",
        "assert len(res[res.order_delivered_customer_date == '2999-12-31']) == 2965"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVbTTB9nuiHH"
      },
      "source": [
        "### Task №8\n",
        "\n",
        "Identify the top 10 sellers with more than 100 orders who most frequently ship their package to other regions (only shipped orders count)\n",
        "\n",
        "Most frequently shipped = highest percentage of orders shipped to another state\n",
        "\n",
        "The table you should have:\n",
        "\n",
        "```\n",
        "seller_id | share\n",
        "\n",
        "  value   | value\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%write_and_run task_8.py\n",
        "\n",
        "# important! all dependencies that you use (if you add new ones) in this class must be explicitly duplicated in this cell\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def task_8(orders, items, sellers, customers):\n",
        "    # merge sellers, items, orders and customers\n",
        "    sellers_items = pd.merge(sellers, items, 'inner', 'seller_id')\n",
        "    orders_sellers_items = pd.merge(orders, sellers_items, 'inner', 'order_id')\n",
        "    data = pd.merge(customers, orders_sellers_items, 'inner', on='customer_id')\n",
        "    # count the number of unique orders from the sellers\n",
        "    sellers_data = data.groupby('seller_id').agg({\"order_id\": \"nunique\"}).reset_index()\n",
        "    # take sellers with more than 100 orders\n",
        "    seller_id_needed = sellers_data[sellers_data['order_id'] > 100]\n",
        "    # taking info on the right sellers\n",
        "    sellers_needed = sellers[sellers['seller_id'].isin(seller_id_needed['seller_id'].values)]\n",
        "    # merge the right sellers and delivered orders\n",
        "    sellers_needed_items = pd.merge(sellers_needed, items, 'inner', 'seller_id')\n",
        "    orders_delivered = orders[orders['order_status'] == 'delivered']\n",
        "    orders_sellers_needed_items = pd.merge(orders_delivered, sellers_needed_items, 'inner', 'order_id')\n",
        "    # add customer info\n",
        "    data_needed = pd.merge(customers, orders_sellers_needed_items, 'inner', on='customer_id')\n",
        "    # make a flag: the sender and the customer are from different states\n",
        "    data_needed['is_other_state'] = (data_needed['seller_state'] != data_needed['customer_state']).astype(int)\n",
        "\n",
        "    # count the proportion of orders with different states of shipper and customer\n",
        "    res = data_needed.drop_duplicates('order_id').groupby('seller_id').agg({\"is_other_state\": \"mean\"}).reset_index()\n",
        "    res = res.rename({'is_other_state': 'share'}, axis=1).sort_values('share', ascending=False)\n",
        "    res = res.iloc[:10].reset_index()\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checks\n",
        "\n",
        "res = task_8(orders, items, sellers, customers)\n",
        "assert np.all(res.share.values == np.sort(res.share.values)[::-1])\n",
        "assert res.share.values[0] == 0.9743589743589743\n",
        "assert res.share.values[-1] == 0.9356435643564357\n",
        "assert res.seller_id.values[5] == '1b4c3a6f53068f0b6944d2d005c9fc89'\n",
        "assert res.seller_id.values[2] == '06a2c3af7b3aee5d69171b0e14f0ee87'\n",
        "assert res.share.sum() == 9.517151493824779"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
